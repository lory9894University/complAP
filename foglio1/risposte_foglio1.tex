\documentclass[a4paper]{article}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{qtree}
\usepackage{xcolor}
\usepackage{forest}
\usepackage{multicol}
\setlength{\columnsep}{3cm}
\usepackage{parskip}
\usepackage{changepage}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[italian]{babel}
\usepackage{thmtools}
\usepackage{mathtools}
\graphicspath{{figures/}}
\newtheorem*{definition}{Def}
\usepackage{xcolor}
\newcommand{\appunto}[1]{\textcolor{ForestGreen}{#1}}
\newcommand{\E}[0]{\mathbb{E}}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}


\begin{document}

\author{Giulia Coucorde, Andrea Cacioli, Lorenzo Dentis 914833}
\title{Risposte Foglio1}
\maketitle
\section{Esercizio 1}
Sia $X$ una v.a che assume il valore $1$ con probabilità $p$ e $(-N)$ con probabilità $1 - p$. Qui $N$ è una v.a. di Poisson di parametro $\lambda$
\begin{align}
	X=
	\begin{cases}
		1 \qquad & p\\
		-N \qquad & 1-p
	\end{cases}
\end{align}
$$N \sim Pois(\lambda)$$ 
\subsection{}
\label{SEC:1.a}
Determinare il valore di $\lambda$ per cui $\E(X) = 0$.\\
Per il teorema dell'attesa totale, $\E[X] = \sum_{i=1}^n\E[X|A_i]P(A_i)$ con $A_1, ..., A_n$ eventi a due a due disgiunti che formano una partizione di E, possiamo affermare che
\begin{align*}
	\E[X] = & P(X=1) * E[1 | X=1] + P(X=-N)*\E[-N | X= -N]=\\
%come abbiamo fatto sparire il condizionamento? v.a. indipendenti?
	= & p + (1-p) * \E[-N]=\\
	  & \text{\textit{essendo N $\sim$ Poisson($\lambda$) vale $\E[N] = \lambda$}}\\
	= & p - \lambda(1-p) 
\end{align*}
$$\lambda = \frac{p}{1-p}$$
\subsection{}
Calcolare $Var(X)$.
Usando la definizione di varianza, $Var(X) = \E[X^2] - \E[X]^2$  ed il valore dell'attesa calcolato in sezione~\ref{SEC:1.a}, scriviamo:
\begin{equation}\label{var}Var(X) =  E[X^2] - [p - \lambda(1-p)]^2\end{equation} 
Analogmantente al punto precedente si può calcolare $\E[X^2]$
\begin{align*}
	\E[X^2] &= p* \E[1^2] + (1-p)\E[(-N)^2]=\\
	%ma glieli mettiamo tutti sti calcoli o li tiriamo via?
		&= p + (1-p)(\lambda + \lambda^2)= p+\lambda +\lambda^2 -p\lambda - p\lambda^2 =\\
		&= p(1-\lambda -\lambda^2) + \lambda(1+\lambda) 
\end{align*}
Sostituendo nella equazione\eqref{var}
$$Var(X) =  p(1-\lambda -\lambda^2) + \lambda(1+\lambda) - (p - \lambda(1-p))^2 $$
\subsection{}
Sia $\{X_i\}i=1,2,...$ una successione di v.a. distribuite come $X$ e sia $Y = \sum_{i=1}^M$, con $M$ v.a. di Poisson di parametro $\beta$, indipendente dalle $X_i$. Determinare $\E(Y)$.


$Y=\sum_{i=1}^M X_i$ \\ $M \sim Pois(\beta)$. \\ Chiamiamo $\E(X_i) = p -\lambda + \lambda p = \mu$\\
$\E(Y)=\sum_{m=1}^{\infty} \E[Y|M=m]P(M=m)$


Data $M \sim Pois(\beta)$ allora $P(M=m) = \frac{\beta^m}{m!} e^{-\beta}$.\\
Invece 
\begin{align*}
	&\E[Y|M=m] = \E[\sum_{i=1}^m] = \textit{ per linearità}\\
	%ditemi voi se si capisce cosa intendo: "quello di sopra è uguale a quello di sotto per linearità"
	& = \sum_{i=1}^m\E[X_i]=m\mu
\end{align*}
Quindi 
\begin{align*}
	\E[y] & = \sum_{m=1}^{\infty} m\mu \qquad \frac{\beta^m}{m!} e^{-\beta}=\\
	      & = e^{-\beta} \mu \sum_{m=1}^{\infty} m\frac{\beta^m}{m!}=\\
	      & = e^{-\beta} \mu \sum_{m=1}^{\infty} \frac{\beta^m}{(m-1)!}=\\
	      & = \textit{ semplificando con Wolfram Alpha } =\\
	      &=\beta \mu
\end{align*}
Questo esercizio poteva essere alternativamente risolto utilizzando il \textit{teorema della doppia attesa} e quindi ponendo 
$\E(Y)=\E[\sum_{i=1}^{N}X_i]=\E[\E[\sum_{i=1}^{N}X_i|N]]$.\\
Condizionando su $N$ e svolgendo i calcoli analogamente a quanto fatto in classe saremmo giunti alla seguente uguaglianza: %scritta così sembra che non abbiamo fatto i calcoli, invece li abbiamo fatti, come posso dirlo diversamente?
$$\E[Y]=\E[\sum_{i=1}^{N}X_i]= \E[N] * \E[X] = \beta \mu$$
\section{Esercizio 2}
Alla stazione di partenza di un treno salgono $K$ persone, con $K$ v.a. distribuita secondo Poisson, di parametro $\lambda = 100$. Il treno effettua un’unica fermata prima dell’arrivo a destinazione. Alla fermata ogni persona scende, con uguale probabilità $p$.


\begin{align*}
	&K \sim Pois(100) \qquad \qquad &X \sim Binom(K,p)\\
	&f_K(k) = \frac{\lambda^k}{k!}e^{-\lambda} &f_X(x) = \binom{k}{x}p^x(1-p)^{k-x}\\
	&\E(k) = \lambda=100 &\E(X) = kp
\end{align*}
Creiamo inoltre una nuova v.a. $Z = K-X$ che conta il numero di persone rimaste sul treno.
\subsection{}
Se nessun nuovo passeggero sale alla fermata intermedia, determinare la probabilità che il treno arrivi alla stazione di destinazione finale con almeno 90 passeggeri.

Si sta cercando $P(Z \geq 90)$.
\begin{align*}
	f_Z(z)=& \sum_{k=0}^{\infty} \underbrace{P(k - X = z | K=k)}_{\binom{k}{k-z}p^{k-z}(1-p)^{z}} \underbrace{P(K=k)}_{\frac{\lambda^k}{k!}e^{-\lambda}} =\\
	       =& \sum_{k=0}^{\infty} \frac{k!}{(k-z)! z!} p^{k-z}(1-p)^{z}\frac{\lambda^k}{k!}e^{-\lambda} = \\
	       =& \textit{portando fuori dalla sommatoria i termini non correlati a k} =\\
	       =& \frac{e^{-\lambda} (1-p)^z}{z!} \sum_{k=0}^{\infty} \frac{k!}{(k-z)!}p^{k-z}\frac{\lambda^k}{k!} = \frac{e^{-\lambda} (1-p)^z}{z!} \sum_{k=0}^{\infty} \frac{p^{k-z}\lambda^k}{(k-z)!} =\\
	       =& \textit{moltiplicando e dividendo $\lambda^{k-z}$} =\\
	       =& \frac{e^{-\lambda} (1-p)^z}{z!} \sum_{k=0}^{\infty} \frac{p^{k-z}\lambda^k}{(k-z)!} * \lambda^{k-z} \lambda^{z-k} =\\ 
	       =&\frac{e^{-\lambda} (1-p)^z \lambda^z}{z!} \sum_{k=0}^{\infty} \frac{p^{k-z}\lambda^k \lambda^{k-z}}{(k-z)! \lambda^k} = \\
	       =& \frac{e^{-\lambda} (1-p)^z \lambda^z}{z!} \underbrace{\sum_{k=0}^{\infty} \frac{(p\lambda)^{k-z}}{(k-z)!}}_{e^{p\lambda}}= \\ %c'è bisogno di spiegare perchè? 
	       =& \frac{e^{p\lambda} e^{-\lambda} (1-p)^z \lambda^z}{z!} = e^{p\lambda - \lambda} \frac{(\lambda - \lambda p)^z}{z!} = Pois(\lambda - \lambda p)
\end{align*}
Quindi $F_Z(z) = \sum_{i=0}^{z} f_Z(i) = P(Z < z)$.\\
$$ P(Z \geq 90) = 1 - P(Z < 90) = 1 - f_Z(89)$$

\subsection{}

\section{Esercizio 3}
\section{Esercizio 4}
\end{document}
