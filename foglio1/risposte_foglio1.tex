\documentclass[a4paper]{article}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{qtree}
\usepackage{xcolor}
\usepackage{forest}
\usepackage{multicol}
\setlength{\columnsep}{3cm}
\usepackage{parskip}
\usepackage{changepage}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[italian]{babel}
\usepackage{thmtools}
\usepackage{mathtools}
\graphicspath{{figures/}}
\newtheorem*{definition}{Def}
\usepackage{xcolor}
\newcommand{\appunto}[1]{\textcolor{ForestGreen}{#1}}
\newcommand{\E}[0]{\mathbb{E}}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}


\begin{document}

\author{Giulia Coucorde, Andrea Cacioli, Lorenzo Dentis lorenzo.dentis@edu.unito.it}
\title{Risposte Foglio1}
\maketitle
\section{Esercizio 1}
Sia $X$ una v.a che assume il valore $1$ con probabilità $p$ e $(-N)$ con probabilità $1 - p$. Qui $N$ è una v.a. di Poisson di parametro $\lambda$
\begin{align}
	X=
	\begin{cases}
		1 \qquad & p\\
		-N \qquad & 1-p
	\end{cases}
\end{align}
$$N \sim Pois(\lambda)$$ 
\subsection{}
\label{SEC:1.a}
Determinare il valore di $\lambda$ per cui $\E(X) = 0$.\\
Per il teorema dell'attesa totale, $\E[X] = \sum_{i=1}^n\E[X|A_i]P(A_i)$ con $A_1, ..., A_n$ eventi a due a due disgiunti che formano una partizione di E, possiamo affermare che
\begin{align*}
	\E[X] = & P(X=1) * E[1 | X=1] + P(X=-N)*\E[-N | X= -N]=\\
%come abbiamo fatto sparire il condizionamento? v.a. indipendenti?
	= & p + (1-p) * \E[-N]=\\
	  & \text{\textit{essendo N $\sim$ Poisson($\lambda$) vale $\E[N] = \lambda$}}\\
	= & p - \lambda(1-p) 
\end{align*}
$$\lambda = \frac{p}{1-p}$$
\subsection{}
Calcolare $Var(X)$.
Usando la definizione di varianza, $Var(X) = \E[X^2] - \E[X]^2$  ed il valore dell'attesa calcolato in sezione~\ref{SEC:1.a}, scriviamo:
\begin{equation}\label{var}Var(X) =  E[X^2] - [p - \lambda(1-p)]^2\end{equation} 
Analogmantente al punto precedente si può calcolare $\E[X^2]$
\begin{align*}
	\E[X^2] &= p* \E[1^2] + (1-p)\E[(-N)^2]=\\
	%ma glieli mettiamo tutti sti calcoli o li tiriamo via?
		&= p + (1-p)(\lambda + \lambda^2)= p+\lambda +\lambda^2 -p\lambda - p\lambda^2 =\\
		&= p(1-\lambda -\lambda^2) + \lambda(1+\lambda) 
\end{align*}
Sostituendo nella equazione\eqref{var}
$$Var(X) =  p(1-\lambda -\lambda^2) + \lambda(1+\lambda) - (p - \lambda(1-p))^2 $$
\subsection{}
Sia $\{X_i\}i=1,2,...$ una successione di v.a. distribuite come $X$ e sia $Y = \sum_{i=1}^M$, con $M$ v.a. di Poisson di parametro $\beta$, indipendente dalle $X_i$. Determinare $\E(Y)$.


$Y=\sum_{i=1}^M X_i$ \\ $M \sim Pois(\beta)$. \\ Chiamiamo $\E(X_i) = p -\lambda + \lambda p = \mu$\\
$\E(Y)=\sum_{m=1}^{\infty} \E[Y|M=m]P(M=m)$


Data $M \sim Pois(\beta)$ allora $P(M=m) = \frac{\beta^m}{m!} e^{-\beta}$.\\
Invece 
\begin{align*}
	&\E[Y|M=m] = \E[\sum_{i=1}^m] = \textit{ per linearità}\\
	%ditemi voi se si capisce cosa intendo: "quello di sopra è uguale a quello di sotto per linearità"
	& = \sum_{i=1}^m\E[X_i]=m\mu
\end{align*}
Quindi 
\begin{align*}
	\E[y] & = \sum_{m=1}^{\infty} m\mu \qquad \frac{\beta^m}{m!} e^{-\beta}=\\
	      & = e^{-\beta} \mu \sum_{m=1}^{\infty} m\frac{\beta^m}{m!}=\\
	      & = e^{-\beta} \mu \sum_{m=1}^{\infty} \frac{\beta^m}{(m-1)!}=\\
	      & = \textit{ semplificando con Wolfram Alpha } =\\
	      &=\beta \mu
\end{align*}
Questo esercizio poteva essere alternativamente risolto utilizzando il \textit{teorema della doppia attesa} e quindi ponendo 
$\E(Y)=\E[\sum_{i=1}^{N}X_i]=\E[\E[\sum_{i=1}^{N}X_i|N]]$.\\
Condizionando su $N$ e svolgendo i calcoli analogamente a quanto fatto in classe saremmo giunti alla seguente uguaglianza: %scritta così sembra che non abbiamo fatto i calcoli, invece li abbiamo fatti, come posso dirlo diversamente?
$$\E[Y]=\E[\sum_{i=1}^{N}X_i]= \E[N] * \E[X] = \beta \mu$$
\section{Esercizio 2}
Alla stazione di partenza di un treno salgono $K$ persone, con $K$ v.a. distribuita secondo Poisson, di parametro $\lambda = 100$. Il treno effettua un’unica fermata prima dell’arrivo a destinazione. Alla fermata ogni persona scende, con uguale probabilità $p$.


\begin{align*}
	&K \sim Pois(100) \qquad \qquad &X \sim Binom(K,p)\\
	&f_K(k) = \frac{\lambda^k}{k!}e^{-\lambda} &f_X(x) = \binom{k}{x}p^x(1-p)^{k-x}\\
	&\E(k) = \lambda=100 &\E(X) = kp
\end{align*}
Creiamo inoltre una nuova v.a. $Z = K-X$ che conta il numero di persone rimaste sul treno.
\subsection{}
Se nessun nuovo passeggero sale alla fermata intermedia, determinare la probabilità che il treno arrivi alla stazione di destinazione finale con almeno 90 passeggeri.

Si sta cercando $P(Z \geq 90)$.
\begin{align*}
	f_Z(z)=& \sum_{k=0}^{\infty} \underbrace{P(k - X = z | K=k)}_{\binom{k}{k-z}p^{k-z}(1-p)^{z}} \underbrace{P(K=k)}_{\frac{\lambda^k}{k!}e^{-\lambda}} =\\
	       =& \sum_{k=0}^{\infty} \frac{k!}{(k-z)! z!} p^{k-z}(1-p)^{z}\frac{\lambda^k}{k!}e^{-\lambda} = \\
	       =& \textit{portando fuori dalla sommatoria i termini non correlati a k} =\\
	       =& \frac{e^{-\lambda} (1-p)^z}{z!} \sum_{k=0}^{\infty} \frac{k!}{(k-z)!}p^{k-z}\frac{\lambda^k}{k!} = \frac{e^{-\lambda} (1-p)^z}{z!} \sum_{k=0}^{\infty} \frac{p^{k-z}\lambda^k}{(k-z)!} =\\
	       =& \textit{moltiplicando e dividendo $\lambda^{k-z}$} =\\
	       =& \frac{e^{-\lambda} (1-p)^z}{z!} \sum_{k=0}^{\infty} \frac{p^{k-z}\lambda^k}{(k-z)!} * \lambda^{k-z} \lambda^{z-k} =\\ 
	       =&\frac{e^{-\lambda} (1-p)^z \lambda^z}{z!} \sum_{k=0}^{\infty} \frac{p^{k-z}\lambda^k \lambda^{k-z}}{(k-z)! \lambda^k} = \\
	       =& \frac{e^{-\lambda} (1-p)^z \lambda^z}{z!} \underbrace{\sum_{k=0}^{\infty} \frac{(p\lambda)^{k-z}}{(k-z)!}}_{e^{p\lambda}}= \\ %c'è bisogno di spiegare perchè? 
	       =& \frac{e^{p\lambda} e^{-\lambda} (1-p)^z \lambda^z}{z!} = e^{p\lambda - \lambda} \frac{(\lambda - \lambda p)^z}{z!} = Pois(\lambda - \lambda p)
\end{align*}
Quindi $F_Z(z) = \sum_{i=0}^{z} f_Z(i) = P(Z < z)$.\\
$$ P(Z \geq 90) = 1 - P(Z < 90) = 1 - f_Z(89)$$

\subsection{}
\label{SEC:2.b}
Se nessun nuovo passeggero sale alla fermata intermedia, determinare il numero medio di passeggeri presenti all’arrivo alla destinazione finale.

Ricordiamo $\E[k] = k = 100$.\\
\begin{align*}
	\E[Z] &=[def] \E[K-X] = \E[K] - \E[X] \textit{ per linearità dell'attesa}\\
	      &= 100 - \sum_{k=1}^{\infty} \E[X|K=k]f_K(k)= \textit{ data $X \sim Binom(k,p)$} = \\
	      &= 100 - \sum_{k=1}^{\infty} \E[kp]f_K(k)= \textit{ essendo k e p due costanti} = \\
	      &= 100 - \sum_{k=1}^{\infty} kpf_K(k)= 100 - p\sum_{k=1}^{\infty} kf_K(k)=
\end{align*}
Per definizione di attesa $ \sum_{k=1}^{\infty} kf_K(k)= \E[k] = \lambda = 100$
$$ \E[Z] = 100 -100p$$
\subsection{}
Si supponga che alla fermata intermedia salga un numero $M$ di passeggeri, con $M$ v.a. indipendente da $K$, v.a. di Poisson di parametro $\beta = 50$. Si determini il numero medio di passeggeri che arriva alla destinazione finale.

$M \sim Pois(\beta)$ con $\beta = 50$ quindi $\E(M) = 50$.\\
Supponendo che ogni passeggero salito alla fermata intermedia non scenda immediatamente $\E[Z + M] = \E[Z] + E[M]$.
Dalla sezione \ref{SEC:2.b} conosciamo $\E[Z] = 100 -100p$, quindi:
$$\E[Z + M]= 100 -100p +50 = 150 - 100p$$
\section{Esercizio 3}

\subsection{} 
Dimostriamo che 
$$\E[X | X] + \E[Y | Y] = X + Y$$
Si noti che per ogni v.a. Z si ha per definizione:
\begin{align}
	\E[Z | Z]=
	\begin{cases}
		\E[Z | Z = z_1] \qquad & P(Z=z_1)\\
		\E[Z | Z = z_2] \qquad & P(Z=z_2)\\
        \vdotswithin{} \notag \\
		\E[Z | Z = z_n] \qquad & P(Z=z_n)\\
	\end{cases}
\end{align}
Ma siccome:
$$
\E[Z | Z = z_1] = \E[z_1] = z_1
$$
Il sistema precedente si puó riscrivere come:
\begin{align}
	\E[Z | Z]=
	\begin{cases}
		z_1 \qquad & P(Z=z_1)\\
		z_2 \qquad & P(Z=z_2)\\
        \vdotswithin{} \notag \\
		z_n \qquad & P(Z=z_n)\\
	\end{cases}
\end{align}
Questa é la definizione di Z, pertanto siccome questo vale per tutte le v.a., vale anche per X e Y. \\
Quindi vero.

\subsection{}
Dimostriamo 
$$\E[X + Y | |X| = x] \ne x + Y$$
Siccome l'attesa della somma é la somma delle attese (Linearitá dell'attesa condizionata):
$$\E[X + Y | |X| = x] = \E[X||X| = x] + \E[Y | |X| = x]$$
La prima attesa puó essere riscritta nel seguente modo separando i due casi possibili del valore assoluto:
$$
\E[X||X| = x] = x \frac{P(X= x)}{P(X= x) + P(X= -x)} - x \frac{P(X= -x)}{P(X= x) + P(X= -x)}=
$$
$$ = x\frac{f_X(x)-f_X(-x)}{f_X(x) + f_X(-x)}$$
Siccome é quindi ovvio che tale risultato é diverso da $x$, vorremmo capire sotto quali ipotesi vale che 
$$x = x\frac{f_X(x)-f_X(-x)}{f_X(x) + f_X(-x)}$$
Svolgendo gli opportuni calcoli, si arriva trova che 
$$\forall x < 0  \ \ \  f_X(x) = 0$$
Sotto queste ipotesi, occorre solamente supporre che
$$\E[Y||X| = x] = Y$$
Questo é vero se $X$ e $Y$ sono indipendenti e $Y = \E[Y]$ \\
Inoltre $Y = \E[Y]$ avviene sempre se per qualche $k$ 
\begin{align*}
	Y =
	\begin{cases}
		k \qquad & p = 1 
	\end{cases}
\end{align*}

Quindi falso in generale. \\
Ma 
\begin{align*}
\E[X + Y | |X| = x] = x + Y \iff
	\begin{cases}
        \text{X e Y sono Indipendenti} \\
        \forall x < 0  \ \ \  f_X(x) = 0 \\
        Y \text{ degenere}
	\end{cases}
\end{align*}


\subsection{}
Dimostriamo che 
$$
\E[X||X|] \ne \E[X | X] 
$$

Come visto al punto 3.a,  $\E[X | X] = X$, inoltre:
\begin{align}
	\E[X | |X|]=
	\begin{cases}
		\E[X | |X| = x_1] \qquad & P(|X|=x_1)\\
		\E[X | |X| = x_2] \qquad & P(|X|=x_2)\\
        \vdotswithin{} \notag \\
		\E[X | |X| = x_n] \qquad & P(|X|=x_n)\\
	\end{cases}
\end{align}

Tuttavia, come visto al punto 3.b, 
$$\E[X | |X| = x] = x\frac{f_X(x)-f_X(-x)}{f_X(x) + f_X(-x)}$$
Inoltre
$$P(|X| = x) \ne P(X = x)$$

Quindi falso. \\ 
Per fare in modo che questa relazione diventi vera, dovremmo trovarci sotto le stesse ipotesi dell'esercizio precedente:
$$
\E[X||X|] = \E[X | X] \iff \forall x<0 \ \ \ f_X(x) = 0
$$

\subsection{}
Vogliamo mostrare che
$$\E[g(X)h(Y) | X] = g(X)\E[h(Y) | X]$$
Per fare ció dobbiamo avvalerci di alcune osservazioni vere quando X e Y sono indipendenti.\\
In particolare abbiamo che:
$$\E[g(X)h(Y) | X] = \E[g(X) | X] \E[h(Y) | X]$$

Ma come visto al punto 3.a (principio di stabilitá attesa condizionata):
$$
\E[g(X) | X] = g(X)
$$
Quindi abbiamo trovato ció che volevamo mostrare ma sotto le condizioni che X e Y siano indipendenti. \\
Quindi:
$$
\E[g(X)h(Y) | X] = g(X)\E[h(Y) | X] \iff X \text{ e } Y \text{ indipendenti}
$$

\subsection{}
Vogliamo mostrare che:
$$
\E[g(X)h(Y) | X = x \ Y = y] = g(x)h(y)
$$
Siccome abbiamo giá una informazione sui valori assunti dalla X e dalla Y, possiamo sostituire e trovare che:
$$
\E[g(x)h(y)] = g(x)h(y)
$$
Questo perché $g(x)h(y)$ é una costante e per ogni costante $k$ abbiamo:
$$
\E[k] = k
$$
Quindi ció che volevamo mostrare é vero.

\subsection{}
Vogliamo mostrare che:
$$
X \text{ e } Y \text{ sono v.a. Gaussiane Indipendenti} \implies \E[XY | X] = XY
$$
Iniziamo a scomporre l'attesa del prodotto nel prodotto delle attese. Si puó fare perché X e Y sono indipendenti:
$$
\E[XY | X] = \E[X | X]\E[Y | X]
$$
Ma come visto prima:
$$
\E[X | X] = X
$$
Non resta che capire cos'é $\E[Y | X]$, seguendo la definizione abbiamo che:
\begin{align}
	\E[Y | X]=
	\begin{cases}
		\E[Y | X = x_1] \qquad & P(X=x_1)\\
		\E[Y | X = x_2] \qquad & P(X=x_2)\\
        \vdotswithin{} \notag \\
		\E[Y | X = x_n] \qquad & P(X=x_n)\\
	\end{cases}
\end{align}

Ma $\E[Y | X = x_1] = \E[Y]$ perché $X$ e $Y$ sono indipendenti.\\
Quindi:
\begin{align}
	\E[Y | X]=
	\begin{cases}
		\E[Y] \qquad & P(X=x_1)\\
		\E[Y] \qquad & P(X=x_2)\\
        \vdotswithin{} \notag \\
		\E[Y] \qquad & P(X=x_n)\\
	\end{cases}
\end{align}
Questo é evidentemente diverso da $Y$ anche sotto le ipotesi fornite dal problema.\\
Occorre aggiungere nuovamente l'ipotesi in cui Y é una v.a. degenere che assume il valore k con probabilitá 1.\\
Tuttavia questo non é possibile in quanto $Y$ é una gaussiana per ipotesi.
\section{Esercizio 4}
\end{document}
