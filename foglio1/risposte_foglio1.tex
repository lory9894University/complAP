\documentclass[a4paper]{article}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{qtree}
\usepackage{xcolor}
\usepackage{forest}
\usepackage{multicol}
\setlength{\columnsep}{3cm}
\usepackage{parskip}
\usepackage{changepage}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[italian]{babel}
\usepackage{thmtools}
\graphicspath{{figures/}}
\newtheorem*{definition}{Def}
\usepackage{xcolor}
\newcommand{\appunto}[1]{\textcolor{ForestGreen}{#1}}
\newcommand{\E}[0]{\mathbb{E}}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}


\begin{document}

\author{Giulia Coucorde, Andrea Cacioli, Lorenzo Dentis 914833}
\title{Risposte Foglio1}
\maketitle
\section{Esercizio 1}
Sia $X$ una v.a che assume il valore $1$ con probabilità $p$ e $(-N)$ con probabilità $1 - p$. Qui $N$ è una v.a. di Poisson di parametro $\lambda$
\begin{align}
	X=
	\begin{cases}
		1 \qquad & p\\
		-N \qquad & 1-p
	\end{cases}
\end{align}
$$N \sim Pois(\lambda)$$ 
\subsection{}
\label{SEC:1.a}
Determinare il valore di $\lambda$ per cui $\E(X) = 0$.\\
Per il teorema dell'attesa totale, $\E[X] = \sum_{i=1}^n\E[X|A_i]P(A_i)$ con $A_1, ..., A_n$ eventi a due a due disgiunti che formano una partizione di E, possiamo affermare che
\begin{align*}
	\E[X] = & P(X=1) * E[1 | X=1] + P(X=-N)*\E[-N | X= -N]=\\
%come abbiamo fatto sparire il condizionamento? v.a. indipendenti?
	= & p + (1-p) * \E[-N]=\\
	  & \text{\textit{essendo N $\sim$ Poisson($\lambda$) vale $\E[N] = \lambda$}}\\
	= & p - \lambda(1-p) 
\end{align*}
$$\lambda = \frac{p}{1-p}$$
\subsection{}
Calcolare $Var(X)$.
Usando la definizione di varianza, $Var(X) = \E[X^2] - \E[X]^2$  ed il valore dell'attesa calcolato in sezione~\ref{SEC:1.a}, scriviamo:
\begin{equation}\label{var}Var(X) =  E[X^2] - [p - \lambda(1-p)]^2\end{equation} 
Analogmantente al punto precedente si può calcolare $\E[X^2]$
\begin{align*}
	\E[X^2] &= p* \E[1^2] + (1-p)\E[(-N)^2]=\\
	%ma glieli mettiamo tutti sti calcoli o li tiriamo via?
		&= p + (1-p)(\lambda + \lambda^2)= p+\lambda +\lambda^2 -p\lambda - p\lambda^2 =\\
		&= p(1-\lambda -\lambda^2) + \lambda(1+\lambda) 
\end{align*}
Sostituendo nella equazione\eqref{var}
$$Var(X) =  p(1-\lambda -\lambda^2) + \lambda(1+\lambda) - (p - \lambda(1-p))^2 $$
\subsection{}
Sia $\{X_i\}i=1,2,...$ una successione di v.a. distribuite come $X$ e sia $Y = \sum_{i=1}^M$, con $M$ v.a. di Poisson di parametro $\beta$, indipendente dalle $X_i$. Determinare $\E(Y)$.


$Y=\sum_{i=1}^M X_i$ \\ $M \sim Pois(\beta)$. \\ Chiamiamo $\E(X_i) = p -\lambda + \lambda p = \mu$\\
$\E(Y)=\sum_{m=1}^{\inf} \E[Y|M=m]P(M=m)$


Data $M \sim Pois(\beta)$ allora $P(M=m) = \frac{\beta^m}{m!} e^{-\beta}$.\\
Invece 
\begin{align*}
	&\E[Y|M=m] = \E[\sum_{i=1}^m] = \textit{ per linearità}\\
	%ditemi voi se si capisce cosa intendo: "quello di sopra è uguale a quello di sotto per linearità"
	& = \sum_{i=1}^m\E[X_i]=m\mu
\end{align*}
Quindi 
\begin{align*}
	\E[y] & = \sum_{m=1}^{\inf} m\mu \qquad \frac{\beta^m}{m!} e^{-\beta}=\\
	      & = e^{-\beta} \mu \sum_{m=1}^{\inf} m\frac{\beta^m}{m!}=\\
	      & = e^{-\beta} \mu \sum_{m=1}^{\inf} \frac{\beta^m}{(m-1)!}=\\
	      & = \textit{ semplificando con Wolfram Alpha } =\\
	      &=\beta \mu
\end{align*}
Questo esercizio poteva essere alternativamente risolto utilizzando il \textit{teorema della doppia attesa} e quindi ponendo 
$\E(Y)=\E[\sum_{i=1}^{N}X_i]=\E[\E[\sum_{i=1}^{N}X_i|N]]$.\\
Condizionando su $N$ e svolgendo i calcoli analogamente a quanto fatto in classe saremmo giunti alla seguente uguaglianza: %scritta così sembra che non abbiamo fatto i calcoli, invece li abbiamo fatti, come posso dirlo diversament %scritta così sembra che non abbiamo fatto i calcoli, invece li abbiamo fatti, come posso dirlo diversamente?
$$\E[Y]=\E[\sum_{i=1}^{N}X_i]= \E[N] * \E[X] = \beta \mu$$
\section{Esercizio 2}
\section{Esercizio 3}
\section{Esercizio 4}
\end{document}
